# Quiz Vulnerability Assessment Framework
## Comprehensive Project Documentation

**Project Owner:** Albert (University of Wollongong)
**Purpose:** Test online quiz vulnerability to GenAI-assisted cheating, aligned with TEQSA 2025 guidance
**Date:** December 2024 - January 2025

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Architecture](#2-architecture)
3. [File Inventory](#3-file-inventory)
4. [Prompt Optimization](#4-prompt-optimization)
5. [Evolution Agent](#5-evolution-agent)
6. [Validation Framework](#6-validation-framework)
7. [Key Insights & Research Findings](#7-key-insights--research-findings)
8. [Setup & Usage](#8-setup--usage)
9. [Known Issues & Solutions](#9-known-issues--solutions)
10. [Next Steps](#10-next-steps)

---

## 1. Project Overview

### Problem Statement

Online quizzes (MCQ, True/False) are vulnerable to AI-assisted cheating. Students can copy questions into ChatGPT or similar tools and receive correct answers. This undermines assessment validity and academic integrity.

### Solution Approach

Build an agentic AI framework that:
1. **Simulates AI-assisted cheating** against live Moodle quizzes
2. **Tests with and without course materials** (RAG) to measure vulnerability
3. **Classifies questions** by cognitive type and vulnerability level
4. **Generates recommendations** for improving question resistance
5. **Iteratively evolves questions** toward AI-resistance (in development)

### Alignment with TEQSA 2025 Guidance

The framework supports educators in creating "AI-irrelevant" assessments by identifying which questions are vulnerable and providing actionable recommendations.

---

## 2. Architecture

### Core Pipeline

```
┌─────────────────────────────────────────────────────────────────────┐
│                        STREAMLIT WEB APP (App.py)                   │
│                     User interface for entire workflow               │
└─────────────────────────────────────────────────────────────────────┘
                                    │
        ┌───────────────────────────┼───────────────────────────┐
        ▼                           ▼                           ▼
┌───────────────┐          ┌───────────────┐          ┌───────────────┐
│   CONNECT     │          │  FIRST SCAN   │          │ SECOND SCAN   │
│   Chrome      │          │  (No RAG)     │          │ (With RAG)    │
│   Debug Port  │          │  Baseline AI  │          │ AI + Materials│
└───────────────┘          └───────────────┘          └───────────────┘
                                    │                           │
                                    └───────────┬───────────────┘
                                                ▼
                                    ┌───────────────────┐
                                    │   MERGE RESULTS   │
                                    │   Compare scores  │
                                    └─────────┬─────────┘
                                              ▼
                                    ┌───────────────────┐
                                    │   REFORM AGENT    │
                                    │   Classify types  │
                                    │   Categorize vuln │
                                    └─────────┬─────────┘
                                              ▼
                                    ┌───────────────────┐
                                    │  ANALYSIS AGENT   │
                                    │  HTML Dashboard   │
                                    │  Recommendations  │
                                    └───────────────────┘
```

### Component Details

| Component | File | Purpose |
|-----------|------|---------|
| Web App | `App.py` | Streamlit interface, workflow orchestration |
| Browser Automation | `quiz_browser_enhanced.py` | Playwright-based Moodle interaction |
| RAG Pipeline | `setup_rag.py`, ChromaDB | Course materials retrieval |
| Classification | `reform_agent.py` | Question type + vulnerability categorization |
| Reporting | `analysis_agent.py` | Statistics, dashboard, recommendations |
| Prompt Testing | `prompt_optimizer.py` | Compare prompt strategies |
| Evolution | `evolution_agent.py` | Iterative question improvement |

### Vulnerability Categories

| Category | No-RAG | With-RAG | Meaning |
|----------|--------|----------|---------|
| **HIGH** | ✓ | ✓ | AI passes both - highly vulnerable |
| **MODERATE** | ✗ | ✓ | RAG helps - course materials increase risk |
| **LOW** | ✗ | ✗ | AI fails both - resistant |
| **ANOMALY** | ✓ | ✗ | RAG hurts - unusual, investigate |

### Question Type Classification (Webb's Depth of Knowledge)

| Type | Description | Typical Vulnerability |
|------|-------------|----------------------|
| RECALL | Simple fact retrieval | HIGH |
| ROUTINE APPLICATION | Standard formulas/procedures | MODERATE-HIGH |
| COMPREHENSION | Understanding concepts | MODERATE |
| ANALYSIS | Breaking down problems | LOW-MODERATE |
| SYNTHESIS | Combining concepts, novel scenarios | LOW |

---

## 3. File Inventory

### Core Application Files

```
~/Documents/quiz-vulnerability/
│
├── App.py                      # Streamlit web interface (main entry point)
├── quiz_browser_enhanced.py    # Browser automation + LLM answering
├── reform_agent.py             # Question classification + vulnerability
├── analysis_agent.py           # Dashboard generation + statistics
├── merge_attempts.py           # Combine no-RAG and with-RAG results
│
├── setup_rag.py                # Initialize ChromaDB with course materials
├── test_rag.py                 # Test RAG retrieval
│
├── prompt_optimizer.py         # Test different prompt strategies
├── optimized_prompt.py         # Optimized prompt variants (v1-v4)
├── evolution_agent.py          # Iterative question evolution
│
├── sample_questions.json       # Test questions for prompt optimization
│
└── chroma_db/                  # ChromaDB vector store (3,358 chunks)
```

### Output Files (Generated)

```
quiz_attempt_YYYYMMDD_HHMMSS_no_rag.json      # First scan results
quiz_attempt_YYYYMMDD_HHMMSS_with_rag.json    # Second scan results
quiz_attempt_YYYYMMDD_HHMMSS.json             # Merged results
quiz_attempt_*_vulnerability_report.json      # Classification + recommendations
quiz_attempt_*_dashboard.html                 # Visual dashboard
quiz_attempt_*_analysis_summary.md            # Markdown report
```

### Configuration

| Setting | Location | Default | Notes |
|---------|----------|---------|-------|
| Default Model | `quiz_browser_enhanced.py` line 84 | `llama3:8b` | Best accuracy |
| Temperature | `quiz_browser_enhanced.py` line 502 | `0` | Deterministic |
| num_predict | `quiz_browser_enhanced.py` line 503 | `512` | Token limit |
| Chrome Debug Port | Various | `9222` | Must match launch command |

---

## 4. Prompt Optimization

### The Problem

Initial prompt achieved only 60% accuracy. Questions appeared more resistant than they actually were, giving false confidence.

### Testing Methodology

Tested 8 prompt strategies against 5 benchmark questions:

| Strategy | Mistral | Llama3:8b | Notes |
|----------|---------|-----------|-------|
| baseline | 60% | 60% | Original simple prompt |
| chain_of_thought | 60% | 60% | Reason before answer |
| elimination | 40% | 80% | Process of elimination |
| expert_role | 40% | 20% | Format failures |
| structured_analysis | 20% | 80% | Good for calculations |
| domain_primed | 0% | 20% | Confused the model |
| confidence_calibrated | 60% | 80% | Consistent format |
| step_by_step | 0% | 0% | Complete format failure |

### Key Findings

1. **Format failures killed accuracy** - Models knew the answer but output "?" instead of letter
2. **Elimination approach** caught conceptual distinctions baseline missed
3. **Domain priming backfired** - Adding definitions confused smaller models
4. **Llama3:8b >> Mistral** - 80% ceiling vs 60% ceiling

### Optimized Prompt (v4) - 100% Accuracy

```
TASK: Answer this multiple choice question correctly.

QUESTION: {question}

OPTIONS:
{options}

ANALYSIS STEPS:

1. QUESTION TYPE: Is this testing recall, conceptual understanding, calculation, or application?

2. KEY INSIGHT: What core concept or distinction is being tested here?

3. EVALUATE OPTIONS:
   A: [KEEP/ELIMINATE] - why?
   B: [KEEP/ELIMINATE] - why?
   C: [KEEP/ELIMINATE] - why?
   D: [KEEP/ELIMINATE] - why?
   E: [KEEP/ELIMINATE] - why?

4. CALCULATION (if needed): Show your working.

5. FINAL SELECTION: From options marked KEEP, select the single best answer.

=== REQUIRED OUTPUT FORMAT ===
After your analysis, you MUST write these three lines:

ANSWER: [write ONE letter: A, B, C, D, or E]
CONFIDENCE: [write a number from 0 to 100]
REASONING: [write one sentence explaining why]

Do not write anything after the REASONING line.

Begin your analysis:
```

### Why v4 Works

| Element | Purpose |
|---------|---------|
| "KEY INSIGHT" step | Catches conceptual distinctions (got Q1 transforming resources right) |
| KEEP/ELIMINATE format | Forces systematic evaluation of each option |
| Explicit format block | "write ONE letter" prevents "?" failures |
| "Do not write anything after" | Prevents trailing garbage breaking parser |

### Improved Answer Parsing

The parser now has fallback strategies:

1. **Primary:** `ANSWER: B`
2. **Alternative patterns:** "the correct answer is B", "select B"
3. **Evaluation fallback:** Find options marked `KEEP`, take last one
4. **Final fallback:** Default to A (with warning)

---

## 5. Evolution Agent

### Concept

An iterative adversarial loop that evolves questions toward AI-resistance:

```
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   Questions → Test → Analyze → Rewrite → Questions'            │
│       ↑                                       │                 │
│       └───────────────────────────────────────┘                 │
│              (repeat until resistant or max iterations)         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                     EVOLUTION CONTROLLER                             │
│  - Manages iteration loop                                            │
│  - Tracks metrics across iterations                                  │
│  - Determines convergence                                            │
└─────────────────────────────────────────────────────────────────────┘
                                │
        ┌───────────────────────┼───────────────────────┐
        ▼                       ▼                       ▼
┌───────────────┐      ┌───────────────┐      ┌───────────────┐
│ STUDENT AGENT │      │ REFORM AGENT  │      │ REWRITE AGENT │
│ (Tests quiz)  │      │ (Analyzes)    │      │ (Evolves Qs)  │
│               │      │               │      │               │
│ - No RAG pass │      │ - Classifies  │      │ - Takes recs  │
│ - RAG pass    │      │ - Categorizes │      │ - Rewrites Q  │
│ - Scores      │      │ - Recommends  │      │ - Validates   │
└───────────────┘      └───────────────┘      └───────────────┘
```

### Convergence Criteria

- **Target:** AI success rate below threshold (default 30%)
- **LOW vulnerability** = AI wrong without RAG AND wrong with RAG
- **Stops when:** Converged, max iterations, or rewrite produces no change

### Usage

```bash
# Basic
python3 evolution_agent.py questions.json

# With options
python3 evolution_agent.py questions.json --iterations 5 --target 20 --model llama3:8b
```

### Output

- `evolution_*_report.md` - Markdown showing each question's journey
- `evolution_*_data.json` - Structured data for further analysis

---

## 6. Validation Framework

### The Problem: Goodhart's Law

> "When a measure becomes a target, it ceases to be a good measure."

Optimizing purely for "AI gets it wrong" creates degenerate solutions:

| Degenerate Solution | Why It "Works" | Why It's Garbage |
|---------------------|----------------|------------------|
| Flip the correct answer | AI now "wrong" 100% | Question is invalid |
| Make question nonsensical | AI can't parse it | Neither can students |
| Add irrelevant complexity | Confuses AI | Confuses everyone |
| Drift from learning objective | Different topic | Not testing what you need |

### Multi-Objective Validation (Required)

Every rewrite must satisfy ALL criteria:

```
┌────────────────────────────────────────┐
│      VALIDATION GAUNTLET               │
│                                        │
│  ☐ Content Validity                    │
│    "Does this still test the           │
│     learning objective?"               │
│                                        │
│  ☐ Construct Validity                  │
│    "Is the marked answer               │
│     actually correct?"                 │
│                                        │
│  ☐ Fairness                            │
│    "Can a knowledgeable                │
│     student answer this?"              │
│                                        │
│  ☐ Clarity                             │
│    "Is it unambiguous?"                │
│                                        │
│  ☐ Semantic Anchoring                  │
│    "Is it still about the              │
│     same concept?"                     │
└────────────────────────────────────────┘
```

### Validation Checks (To Implement)

1. **Answer Verification**
   - Ask LLM: "Explain why [marked correct] is the right answer"
   - If it can't justify → question is broken
   - If it justifies different answer → wrong answer marked

2. **Knowledgeable Student Test**
   - Test with "domain expert" prompt
   - If expert-mode fails → question may be unfair/broken
   - If expert passes, student-mode fails → good! Requires actual knowledge

3. **Learning Objective Alignment**
   - "Does this question test [learning objective]?" Score 1-5
   - Reject if below 4

4. **Semantic Drift Check**
   - Measure similarity between original and rewritten
   - Reject if drift too high

### Design Decisions

| Question | Decision |
|----------|----------|
| Auto-reject bad rewrites? | Yes, if clearly fails validation criteria |
| "Cannot be made AI-resistant" valid outcome? | Yes - important research finding |
| Minimum validity bar | Correct answer must be justifiable |

### The Uncomfortable Truth

Some learning objectives are **fundamentally AI-vulnerable** in MCQ format. Valid conclusion:

> "This concept cannot be validly assessed with auto-gradeable MCQ if AI-resistance is required. Consider short-answer or application-based assessment."

This is not a failure - it's a valuable finding about the limits of auto-gradeable assessment.

---

## 7. Key Insights & Research Findings

### Prompt Engineering

1. **Elimination beats direct answering** - Forcing systematic option evaluation catches conceptual distinctions
2. **Format enforcement is critical** - Models often know the answer but fail to output it correctly
3. **Domain priming can backfire** - Adding definitions confused smaller models
4. **Model choice matters significantly** - Llama3:8b ceiling 100%, Mistral ceiling 60%

### Assessment Design

1. **MCQ has inherent vulnerability ceiling** - Some concepts cannot be AI-resistant in MCQ format
2. **Question type predicts vulnerability** - RECALL almost always HIGH, SYNTHESIS usually LOW
3. **RAG impact varies** - Some questions MORE vulnerable with course materials, some LESS

### Adversarial Evolution

1. **Pure optimization is dangerous** - Must validate rewrites against multiple objectives
2. **Convergence is not guaranteed** - Some questions may never reach LOW vulnerability
3. **"Cannot be fixed" is a valid outcome** - Informs assessment format decisions

### Technical Lessons

1. **Temperature=0 essential** for consistency testing
2. **subprocess.Popen with streaming can hang Streamlit** - use subprocess.run with capture_output
3. **Scores must flow through entire pipeline** - session state → merge → reform → analysis

---

## 8. Setup & Usage

### Prerequisites

```bash
# Python packages
pip install streamlit playwright ollama chromadb

# Playwright browsers
playwright install chromium

# Ollama models
ollama pull llama3:8b
ollama pull llava  # For image interpretation
```

### Launch Chrome with Debug Port

```bash
# macOS
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome \
  --remote-debugging-port=9222 \
  --user-data-dir=/tmp/chrome-debug

# Linux
google-chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-debug

# Windows
"C:\Program Files\Google\Chrome\Application\chrome.exe" ^
  --remote-debugging-port=9222 --user-data-dir=%TEMP%\chrome-debug
```

### Run Web App

```bash
cd ~/Documents/quiz-vulnerability
python3 -m streamlit run App.py
```

### Workflow

1. **Connect tab:** Connect to Chrome (must be on quiz page)
2. **First Scan tab:** Run baseline AI test → Submit in Moodle → Collect results
3. **Second Scan tab:** Start new attempt → Run AI+RAG test → Submit → Collect
4. **Results tab:** View summary → Generate full report → View dashboard

### Command Line Tools

```bash
# Test prompt strategies
python3 prompt_optimizer.py --model llama3:8b

# Run evolution
python3 evolution_agent.py questions.json --iterations 3

# Direct quiz scan (without web app)
python3 quiz_browser_enhanced.py --no-rag --model llama3:8b
python3 quiz_browser_enhanced.py --with-rag --model llama3:8b
```

---

## 9. Known Issues & Solutions

### Issue: Chrome Connection Hangs

**Symptom:** App freezes on "Connecting to browser..."

**Solution:** Ensure Chrome is running with debug port BEFORE starting app:
```bash
# Kill existing Chrome instances first
pkill -f "Google Chrome"
# Then launch with debug port
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome \
  --remote-debugging-port=9222 --user-data-dir=/tmp/chrome-debug
```

### Issue: Dashboard Shows Wrong Scores

**Symptom:** Summary shows different scores than dashboard

**Cause:** Scores weren't flowing through the entire pipeline

**Solution:** Fixed in current version - scores now passed through:
- `App.py` → `merge_attempts()` (session state scores)
- `reform_agent.py` → passes through `actual_scores`
- `analysis_agent.py` → uses `actual_scores` if present

### Issue: Inconsistent LLM Answers

**Symptom:** Same question gets different answers on repeated runs

**Solution:** 
- Set `temperature=0` in ollama.chat options
- Use `llama3:8b` instead of `mistral`
- Use optimized v4 prompt

### Issue: Activity Log Not Updating

**Symptom:** No progress feedback during scans

**Solution:** Current version parses subprocess stdout after completion and logs question-level activity

### Issue: Old Files Causing Confusion

**Symptom:** Multiple JSON files with different timestamps

**Solution:** Clean up before new run:
```bash
rm -f quiz_attempt_*.json *_dashboard.html *_vulnerability_report.json
```

---

## 10. Next Steps

### Immediate (Ready to Implement)

1. **Validation Framework for Evolution Agent**
   - Add validation gauntlet before accepting rewrites
   - Implement answer verification check
   - Add "cannot be made AI-resistant" as valid conclusion

2. **Integration with Streamlit App**
   - Add "Evolution" tab to web app
   - Show iteration-by-iteration progress
   - Allow human review of proposed rewrites

### Short-Term

3. **Expand Test Question Bank**
   - Add more questions from actual course quizzes
   - Test across different subjects/domains
   - Build benchmark dataset

4. **RAG Optimization**
   - Test different chunk sizes
   - Experiment with retrieval strategies
   - Measure RAG impact by question type

### Research Directions

5. **Limits of MCQ AI-Resistance**
   - Which learning objectives CAN be AI-resistant as MCQ?
   - What question characteristics predict resistance?
   - Where should educators switch to other formats?

6. **Adversarial Training Dataset**
   - Collect questions that successfully converged to LOW
   - Analyze patterns in effective rewrites
   - Build recommendations taxonomy

7. **Publication Potential**
   - "Automated Assessment of AI Vulnerability in Online Quizzes"
   - "Limits of AI-Resistant Auto-Gradeable Assessment"
   - "Adversarial Evolution of Assessment Items"

---

## Appendix: Model Comparison

| Model | Size | RAM | Accuracy | Speed | Recommendation |
|-------|------|-----|----------|-------|----------------|
| `mistral` | 7B | ~4GB | 60% | Fast | Not recommended |
| `llama3:8b` | 8B | ~5GB | 100% | Medium | **Recommended** |
| `gemma2:9b` | 9B | ~6GB | ~80% | Medium | Alternative |
| `mixtral` | 8x7B | ~26GB | ~90% | Slow | If resources allow |
| `llava` | 7B | ~4GB | N/A | Medium | Image interpretation only |

---

## Appendix: Project Location

```
Primary: ~/Documents/quiz-vulnerability/
RAG DB:  ~/Documents/quiz-vulnerability/chroma_db/
Outputs: Generated in working directory
```

---

*Document generated for project continuation. Import this alongside all Python files into new chat.*
